[Main]

# ========================================================================== #
# Monitoring Agent
# ========================================================================== #
# Monitoring API URL: URL for the monitoring API
#mon_api_url: https://region-a.geo-1.monitoring.hpcloudsvc.com/v1.1/metrics
mon_api_url: http://192.168.10.4:8080/v2.0/metrics
# Monitoring API Project Id: Project Id for the monitoring API
mon_api_project_id: 82510970543135
# Monitoring API Username: Username for the monitoring API
mon_api_username: 64564238516478
# Monitoring API Password: Password for the monitoring API
mon_api_password: Computer#123
# Use Keystone for Authentication?: Use Keystone for authentication
# True or False
use_keystone: False
# Keystone API URL: URL for the Keystone server to use
keystone_url: https://region-a.geo-1.identity.hpcloudsvc.com:35357/v3/auth/tokens
# Aggregate Metrics?: Send single message or message for each metric
# True or False
aggregate_metrics: True

# Name mappings from DD to Monitoring Agent names
agentVersion_mapping: DISCARD
apiKey_mapping: DISCARD
collection_timestamp_mapping: DISCARD
cpuIdle_mapping: cpu_idle_perc
cpuStolen_mapping: cpu_stolen_perc
cpuSystem_mapping: cpu_system_perc
cpuWait_mapping: cpu_iowait_perc
events_mapping: DISCARD
host-tags_mapping: DISCARD
internalHostname_mapping: DISCARD
rkB/s_mapping: io_read_kbytes_sec
avgrq-sz_mapping: DISCARD
svctm_mapping: DISCARD
r/s_mapping: io_read_req_sec
wrqm/s_mapping: DISCARD
wkB/s_mapping: io_write_kbytes_sec
r_await_mapping: DISCARD
w_await_mapping: DISCARD
rrqm/s_mapping: DISCARD
w/s_mapping: io_write_req_sec
await_mapping: DISCARD
avgqu-sz_mapping: DISCARD
memPhysTotal_mapping: mem_total_mb
memPhysFree_mapping: mem_free_mb
memPhysUsable_mapping: mem_usable_mb
memPhysPctUsable_mapping: mem_usable_perc
memPhysUsed_mapping: mem_usable_perc
memBuffers_mapping: mem_used_buffers
memCached_mapping: mem_used_cached
memShared_mapping: mem_used_shared
memSwapPctFree_mapping: mem_swap_free_perc
memSwapTotal_mapping: mem_swap_total_mb
memSwapUsed_mapping: mem_swap_used_mb
memSwapFree_mapping: mem_swap_free_mb
meta_mapping: DISCARD
os_mapping: DISCARD
processes_mapping: DISCARD
python_mapping: DISCARD
system.io.bytes_per_s_mapping: disk_io_bytes_sec
system.load.1_mapping: load_avg_1_min
system.load.5_mapping: load_avg_5_min
system.load.15_mapping: load_avg_15_mi
system.load.norm.1_mapping: DISCARD
system.load.norm.5_mapping: DISCARD
system.load.norm.15_mapping: DISCARD
systemStats_mapping: DISCARD
uuid_mapping: DISCARD
#%util_mapping: DISCARD

# Force the hostname to whatever you want.
# hostname: my_server

# Set the host's tags
tags: Origin:Mini-mon, Environment:Dev

# ========================================================================== #
# Custom Emitters                                                            #
# ========================================================================== #

# Comma-separated list of emitters to be used in addition to the standard one
#
# Expected to be passed as a comma-separated list of colon-delimited
# name/object pairs.
#
#custom_emitters: /Users/gary/cloudos/mon-agent/monagent/monapi/mon_api_emitter.py:MonApiEmitter
#
# If the name of the emitter function is not specified, 'emitter' is assumed.

# ========================================================================== #
# Logging
# ========================================================================== #

log_level: DEBUG

collector_log_file: /var/log/mon-agent/collector.log
forwarder_log_file: /var/log/mon-agent/forwarder.log
#dogstatsd_log_file: /var/log/mon-agent/dogstatsd.log
# pup_log_file:       /var/log/mon-agent/pup.log

# if syslog is enabled but a host and port are not set, a local domain socket
# connection will be attempted
#
# log_to_syslog: yes
# syslog_host:
# syslog_port:
disable_file_logging: True

send_to_datadog: no
dd_url: https://app.datadoghq.com
api_key: f218f9096f5a09929f65718d50a817fa
collect_ec2_tags: no

# Set the threshold for accepting points to allow anything
# with recent_point_threshold seconds
# Defaults to 30 seconds if no value is provided
#recent_point_threshold: 30

# Use mount points instead of volumes to track disk and fs metrics
use_mount: no

# Change port the Agent is listening to
# listen_port: 17123

# Start a graphite listener on this port
# graphite_listen_port: 17124

# Additional directory to look for Datadog checks
# additional_checksd: /etc/dd-agent/checks.d/

# Allow non-local traffic to this Agent
# This is required when using this Agent as a proxy for other Agents
# that might not have an internet connection
# For more information, please see
# https://github.com/DataDog/dd-agent/wiki/Network-Traffic-and-Proxy-Configuration
# non_local_traffic: no

# ========================================================================== #
# Pup configuration
# ========================================================================== #

# Pup is a small server that displays metric data collected by the Agent.
# Think of it as a fancy status page or a toe dip into the world of
# datadog. It can be connected to on the port below.

use_pup: no
# pup_port: 17125
# pup_interface: localhost
# pup_url: http://localhost:17125

# ========================================================================== #
# DogStatsd configuration                                                    #
# ========================================================================== #

# DogStatsd is a small server that aggregates your custom app metrics. For
# usage information, check out http://api.datadoghq.com

#  Make sure your client is sending to the same port.
# dogstatsd_port : 8125

# By default dogstatsd will post aggregate metrics to the Agent (which handles
# errors/timeouts/retries/etc). To send directly to the datadog api, set this
# to https://app.datadoghq.com.
# dogstatsd_target : http://localhost:17123

## The dogstatsd flush period.
# dogstatsd_interval : 10

## If 'yes', counters and rates will be normalized to 1 second (that is divided
## by the dogstatsd_interval) before being sent to the server. Defaults to 'yes'
# dogstatsd_normalize : yes

# If you want to forward every packet received by the dogstatsd server
# to another statsd server, uncomment these lines.
# WARNING: Make sure that forwarded packets are regular statsd packets and not "dogstatsd" packets,
# as your other statsd server might not be able to handle them.
# statsd_forward_host: address_of_own_statsd_server
# statsd_forward_port: 8125

# ========================================================================== #
# Service-specific configuration                                             #
# ========================================================================== #

# -------------------------------------------------------------------------- #
#   Disk                                                                     #
# -------------------------------------------------------------------------- #

# Some infrastrucures have many constantly changing virtual devices (e.g. folks
# running constantly churning linux containers) whose metrics aren't
# interesting for datadog. To filter out a particular pattern of devices
# from collection, configure a regex here:
# device_blacklist_re: .*\/dev\/mapper\/lxc-box.*

# -------------------------------------------------------------------------- #
#   Ganglia                                                                  #
# -------------------------------------------------------------------------- #

# Ganglia host where gmetad is running
#ganglia_host: localhost

# Ganglia port where gmetad is running
#ganglia_port: 8651

# -------------------------------------------------------------------------- #
#   Cassandra                                                                #
# -------------------------------------------------------------------------- #
#cassandra_host: localhost
#cassandra_nodetool: /usr/bin/nodetool
## If you're using Cassandra 0.8 or higher:
# cassandra_port: 7199
## If you're using Cassandra 0.7 or lower:
#cassandra_port: 8080


# -------------------------------------------------------------------------- #
#   Nagios                                                                   #
# -------------------------------------------------------------------------- #

# dd-agent imports alerts and perfdata from nagios.

# Path to Nagios' event log file
# Make sure the dd-agent user can read this file
#nagios_log: /var/log/nagios3/nagios.log

# If you use perfdata, dd-agent can import automatically and in real-time
# performance data collected by nagios.
# For more information on perfdata configuration, please refer to
# http://nagios.sourceforge.net/docs/3_0/perfdata.html
#
# Path to Nagios' ***configuration*** file where the properties
# host|service_perfdata_file and host|service_perfdata_file_template
# are defined.
# (ubuntu 10.04)
#nagios_perf_cfg: /etc/nagios3/nagios.cfg
# (centos 5)
#nagios_perf_cfg: /etc/nagios/nagios.cfg

# -------------------------------------------------------------------------- #
#  Dogstream (log file parser)
# -------------------------------------------------------------------------- #

# Comma-separated list of logs to parse and optionally custom parsers to use.
# The form should look like this:
#
#   dogstreams: /path/to/log1:parsers_module:custom_parser, /path/to/log2, /path/to/log3, ...
#
# Or this:
#
#   dogstreams: /path/to/log1:/path/to/my/parsers_module.py:custom_parser, /path/to/log2, /path/to/log3, ...
#
# Each entry is a path to a log file and optionally a Python module/function pair
# separated by colons.
#
# Custom parsers should take a 2 parameters, a logger object and
# a string parameter of the current line to parse. It should return a tuple of
# the form:
#   (metric (str), timestamp (unix timestamp), value (float), attributes (dict))
# where attributes should at least contain the key 'metric_type', specifying
# whether the given metric is a 'counter' or 'gauge'.
#
# Unless parsers are specified with an absolute path, the modules must exist in
# the Agent's PYTHONPATH. You can set this as an environment variable when
# starting the Agent. If the name of the custom parser function is not passed,
# 'parser' is assumed.
#
# If this value isn't specified, the default parser assumes this log format:
#     metric timestamp value key0=val0 key1=val1 ...
#


